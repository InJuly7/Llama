import torch


class StaticCache:
    """æ¯ä¸€å±‚éƒ½ä¼šåˆ›å»ºè‡ªå·±çš„StaticCacheå®ä¾‹"""

    def __init__(self, max_batch_size, num_heads, max_seq_len, head_dim):
        self.key_cache = torch.zeros(max_batch_size, num_heads, max_seq_len, head_dim)
        self.value_cache = torch.zeros(max_batch_size, num_heads, max_seq_len, head_dim)

    def update(self, key_states, value_states, layer_idx, cache_kwargs):
        cache_position = cache_kwargs["cache_position"]

        start_pos = cache_position[0].item()
        seq_len = key_states.shape[2]
        end_pos = start_pos + seq_len

        # å†™å…¥æŒ‡å®šä½ç½®
        self.key_cache[:, :, start_pos:end_pos, :] = key_states
        self.value_cache[:, :, start_pos:end_pos, :] = value_states

        return (self.key_cache[:, :, :end_pos, :], self.value_cache[:, :, :end_pos, :])


# =================== å®Œæ•´Demo ===================
class TransformerWithStaticCache:
    def __init__(self, num_layers=3, num_heads=8, head_dim=64, max_seq_len=100):
        self.num_layers = num_layers

        # ğŸ”¥å…³é”®ï¼šæ¯ä¸€å±‚éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„Cacheå®ä¾‹
        self.caches = [
            StaticCache(max_batch_size=1, num_heads=num_heads, max_seq_len=max_seq_len, head_dim=head_dim) for _ in range(num_layers)
        ]

    def forward(self, input_ids, cache_position):
        """
        cache_position: å‘Šè¯‰æ¯ä¸€å±‚, å½“å‰tokenåº”è¯¥å†™å…¥cacheçš„å“ªä¸ªä½ç½®
        """
        batch_size, seq_len = input_ids.shape
        num_heads = 8
        head_dim = 64

        # æ¨¡æ‹Ÿæ¯ä¸€å±‚çš„å¤„ç†
        for layer_idx in range(self.num_layers):
            print(f"\n--- Layer {layer_idx} ---")

            # æ¨¡æ‹Ÿå½“å‰å±‚è®¡ç®—å‡ºçš„key/value
            key_states = torch.randn(batch_size, num_heads, seq_len, head_dim)
            value_states = torch.randn(batch_size, num_heads, seq_len, head_dim)

            # ğŸ”¥ æ¯å±‚ä½¿ç”¨è‡ªå·±çš„cache
            cache_kwargs = {"cache_position": cache_position}
            cached_key, cached_value = self.caches[layer_idx].update(key_states, value_states, layer_idx, cache_kwargs)

            print(f"Input key shape: {key_states.shape}")
            print(f"Cached key shape: {cached_key.shape}")
            print(f"Cache position: {cache_position}")


# =================== ä½¿ç”¨ç¤ºä¾‹ ===================
model = TransformerWithStaticCache(num_layers=3)

print("=" * 60)
print("é˜¶æ®µ1: Prefill - å¤„ç†prompt 'Hello world' (å‡è®¾5ä¸ªtoken)")
print("=" * 60)
input_ids = torch.tensor([[1, 2, 3, 4, 5]])  # shape: [1, 5]
cache_position = torch.arange(0, 5)  # [0, 1, 2, 3, 4]
model.forward(input_ids, cache_position)

print("\n" + "=" * 60)
print("é˜¶æ®µ2: Decode - ç”Ÿæˆç¬¬1ä¸ªtoken (ä½ç½®5)")
print("=" * 60)
new_token = torch.tensor([[6]])  # shape: [1, 1]
cache_position = torch.tensor([5])  # å†™å…¥ä½ç½®5
model.forward(new_token, cache_position)

print("\n" + "=" * 60)
print("é˜¶æ®µ3: Decode - ç”Ÿæˆç¬¬2ä¸ªtoken (ä½ç½®6)")
print("=" * 60)
new_token = torch.tensor([[7]])
cache_position = torch.tensor([6])  # å†™å…¥ä½ç½®6
model.forward(new_token, cache_position)

"""
Output Log:
============================================================
é˜¶æ®µ1: Prefill - å¤„ç†prompt 'Hello world' (å‡è®¾5ä¸ªtoken)
============================================================

--- Layer 0 ---
Input key shape: torch.Size([1, 8, 5, 64])
Cached key shape: torch.Size([1, 8, 5, 64])
Cache position: tensor([0, 1, 2, 3, 4])

--- Layer 1 ---
Input key shape: torch.Size([1, 8, 5, 64])
Cached key shape: torch.Size([1, 8, 5, 64])
Cache position: tensor([0, 1, 2, 3, 4])

--- Layer 2 ---
Input key shape: torch.Size([1, 8, 5, 64])
Cached key shape: torch.Size([1, 8, 5, 64])
Cache position: tensor([0, 1, 2, 3, 4])

============================================================
é˜¶æ®µ2: Decode - ç”Ÿæˆç¬¬1ä¸ªtoken (ä½ç½®5)
============================================================

--- Layer 0 ---
Input key shape: torch.Size([1, 8, 1, 64])
Cached key shape: torch.Size([1, 8, 6, 64])
Cache position: tensor([5])

--- Layer 1 ---
Input key shape: torch.Size([1, 8, 1, 64])
Cached key shape: torch.Size([1, 8, 6, 64])
Cache position: tensor([5])

--- Layer 2 ---
Input key shape: torch.Size([1, 8, 1, 64])
Cached key shape: torch.Size([1, 8, 6, 64])
Cache position: tensor([5])

============================================================
é˜¶æ®µ3: Decode - ç”Ÿæˆç¬¬2ä¸ªtoken (ä½ç½®6)
============================================================

--- Layer 0 ---
Input key shape: torch.Size([1, 8, 1, 64])
Cached key shape: torch.Size([1, 8, 7, 64])
Cache position: tensor([6])

--- Layer 1 ---
Input key shape: torch.Size([1, 8, 1, 64])
Cached key shape: torch.Size([1, 8, 7, 64])
Cache position: tensor([6])

--- Layer 2 ---
Input key shape: torch.Size([1, 8, 1, 64])
Cached key shape: torch.Size([1, 8, 7, 64])
Cache position: tensor([6])
"""
